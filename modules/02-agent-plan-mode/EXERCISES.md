# Module 2: Agent Plan Mode ‚Äî Exercises

## ‚è∞ Monday 10:30 AM ‚Äî Planning Before Implementing

> *"Before we start generating code, let's plan what we're doing. The best AI results come from the clearest thinking."*  
> ‚Äî Sarah, introducing the team to structured AI collaboration

---

## üìñ Story So Far

In **Module 00**, the FanHub team struggled with the Character Detail Challenge‚Äîundefined architecture, inconsistent patterns, generic AI output led to 4-5 attempts and frustration.

In **Module 01**, they added structure: `docs/ARCHITECTURE.md` documented the system design, and `.github/copilot-instructions.md` established coding standards. The same challenge became dramatically easier‚Äî2 attempts, better consistency, clear patterns.

But Sarah noticed something: **even with better context, the team still jumps straight to implementation**. They ask Copilot to build, get code, realize they missed something, iterate. It works, but it's reactive.

**Now, in Module 02**, Sarah introduces **plan mode thinking**‚Äîusing AI to structure your thinking BEFORE implementation. This isn't about creating project management overhead. It's about leveraging AI for the hardest part of development: **figuring out what to build and how to build it.**

---

üí° **Understanding Plan Mode vs Implementation Mode**

GitHub Copilot operates in two distinct modes when you use the `@agent` prefix:

**Plan Mode**: Focus on analysis, strategy, and structured thinking
- Asks clarifying questions about requirements
- Identifies potential approaches and tradeoffs
- Creates detailed action plans
- Highlights considerations and edge cases
- Outputs structured planning documents

**Implementation Mode**: Focus on execution and code generation
- Generates specific code solutions
- Makes concrete changes to files
- Provides ready-to-use implementations
- Assumes planning decisions have been made

**When to use plan mode**: Problem exploration, approach comparison, configuration design, architecture decisions, workflow optimization, debugging strategy

**When to use implementation mode**: After planning is complete, when requirements are clear, for tactical code generation

---

## üéØ Learning Objectives

By the end of this module, you will:

- Understand the difference between agent plan mode and implementation mode
- Use plan mode to ask clarifying questions before implementing
- Save planning results to files for team collaboration
- Apply iterative planning workflows to both code and Copilot configuration
- Experience how structured thinking improves AI output quality

**Time**: ~90 minutes  
**Featured Personas**: Sarah (Skeptical Senior), David (Seasoned Architect), Marcus (DevOps Developer)

---

## üß† Mindful Moment: The Planning Paradox

Traditional thinking: *"Planning slows us down‚Äîlet's just start coding."*

AI-native thinking: *"Planning speeds us up‚ÄîAI helps us think, not just type."*

This isn't about creating bureaucracy. It's about leveraging AI for the hardest part of development: **figuring out what to build and how to build it.** The best AI results come from the clearest human thinking. Plan mode helps you think clearly.

---

## üìö Key Concepts

### Agent Plan Mode vs Implementation Mode

GitHub Copilot operates in two distinct modes when you use the `@agent` prefix:

**Plan Mode**: Focus on analysis, strategy, and structured thinking
- Asks clarifying questions
- Identifies potential approaches
- Creates detailed action plans
- Highlights considerations and tradeoffs
- Outputs structured planning documents

**Implementation Mode**: Focus on execution and code generation
- Generates specific code solutions
- Makes concrete changes to files
- Provides ready-to-use implementations
- Assumes planning decisions have been made

### When to Use Plan Mode

Plan mode excels at:
- **Problem exploration** ‚Äî Understanding requirements before implementing
- **Approach comparison** ‚Äî Evaluating multiple solutions
- **Configuration planning** ‚Äî Designing Copilot customizations
- **Architecture decisions** ‚Äî Thinking through system design
- **Workflow optimization** ‚Äî Planning development processes

### The Planning-to-Implementation Flow

The most effective AI collaboration follows this pattern:
1. **Plan**: Use `@agent` to think through the problem
2. **Save**: Store the plan in a file for reference
3. **Implement**: Switch to implementation mode with clear direction
4. **Iterate**: Return to planning when you need to adjust course

---

## üî® Exercises

### Exercise 2.1: Plan Before You Code ‚Äî "The Character Detail Challenge: Final Form"

> üßµ **The Golden Thread Continues**: You've attempted this challenge in Module 00 (chaos), Module 01a (better structure), and Module 01b (consistent patterns). Now see what happens when you **plan first**.

**‚è±Ô∏è Time**: ~25 minutes  
**Lead Persona**: Sarah (Skeptical Senior)  
**Supporting Cast**: David (architecture review), Priya (implementation), Marcus (CI/CD perspective)

#### üìñ The Story

**Sarah** (Skeptical Senior, 15 years) has been tracking the team's progress on the Character Detail Challenge. She's noticed something: even with architecture docs and coding standards, they still jump straight to implementation‚Äîand sometimes miss important considerations.

*"We're treating AI like a magic code machine,"* Sarah observes. *"But the best results happen when we use it to think first, then implement."*

Today, Sarah will demonstrate how plan mode completes the transformation‚Äîturning the Character Detail Challenge from a struggle into a systematic success.

**[See Sarah's full journey ‚Üí](personas/sarah.md)**

#### ‚ùå The "Before" ‚Äî What We've Been Doing

Even with documentation in place, you've been asking for implementation directly:

```
@workspace Add a character detail page that shows:
- Character name
- Photo
- Biography
- List of episodes they appear in
```

This approach jumps straight to code. But consider:
- What happens if a character has no photo? No biography?
- How should this page integrate with navigation?
- What about loading states? Error states?
- Should users be able to edit character info? Mark as favorite?
- How does this connect to the episode list we might add later?

**Without planning, these questions surface AFTER you've written the code.**

#### üéØ Objective

Use agent plan mode to structure your thinking before implementing the Character Detail page, creating a comprehensive plan that leads to first-try success.

#### üìã Steps

1. **Review your previous attempts**
   
   Open `docs/character-detail-challenge.md` and review your metrics from:
   - Module 00: The Struggle (no context)
   - Module 01a: After ARCHITECTURE.md
   - Module 01b: After copilot-instructions.md
   
   Notice that even with docs and standards, you may have needed multiple iterations.

2. **Switch to plan mode for structured thinking**
   
   Start a new chat conversation and use the `@agent` prefix:
   
   ```
   @agent I need to add a character detail page to the FanHub app.
   Before we implement anything, help me create a comprehensive plan.
   
   Please analyze the current codebase and create a structured plan that addresses:
   
   1. Data requirements - what character information do we have? What's missing?
   2. API needs - do we have the right endpoints? What might we need to add?
   3. Component structure - how should this page be organized?
   4. Integration points - how does this connect to navigation, other pages?
   5. Edge cases - no photo, long biography, character in zero episodes?
   6. User experience - loading states, error handling, mobile responsiveness?
   7. Future considerations - favorites, editing, social sharing?
   
   Ask me clarifying questions if you need more context about our specific requirements.
   ```

3. **Engage with the clarifying questions**
   
   Plan mode will likely ask you questions like:
   - Is character data editable or read-only?
   - Should we show related characters (same show)?
   - Do we need breadcrumb navigation back to the character list?
   - Should there be a "favorite" or "bookmark" feature?
   - How should we handle characters with extensive episode appearances?
   
   **This is where your product thinking matters most.** Answer thoughtfully‚Äîthese decisions shape the implementation.

4. **Generate the structured plan**
   
   Once you've clarified the requirements, ask for the complete plan:
   
   ```
   Based on our discussion, create a detailed implementation plan with:
   - Data model validation (what exists vs. what we need)
   - API endpoint specifications (existing + any new ones)
   - Component hierarchy and props
   - State management approach
   - Testing strategy
   - Implementation order (what to build first)
   
   Format this as a document I can save and share with the team.
   ```

5. **Save the plan for team collaboration**
   
   Create a new file to store your planning results:
   
   ```
   Create fanhub/docs/FEATURE-PLAN-character-detail.md with the complete plan
   ```
   
   This becomes your implementation guide and helps other team members understand your approach.

6. **Now implement with the plan as context**
   
   With your plan saved, ask for implementation:
   
   ```
   @workspace Based on the plan in docs/FEATURE-PLAN-character-detail.md, 
   implement the CharacterDetail page component. Start with the core layout 
   and data fetching as specified in the plan.
   ```

7. **Update your tracking document**
   
   Open `docs/character-detail-challenge.md` and complete the final section:
   
   | Metric | Mod 00 | Mod 01a | Mod 01b | **Mod 02** |
   |--------|--------|---------|---------|------------|
   | Time to working code | ___ | ___ | ___ | ___ |
   | Prompt attempts | ___ | ___ | ___ | ___ |
   | Edge cases identified | ___ | ___ | ___ | ___ |
   | Confidence (1-10) | ___ | ___ | ___ | ___ |

#### ‚úÖ Success Criteria

- [ ] Used `@agent` prefix to access plan mode
- [ ] Engaged with clarifying questions about the character detail page
- [ ] Generated a comprehensive implementation plan
- [ ] Saved the plan to `fanhub/docs/FEATURE-PLAN-character-detail.md`
- [ ] Implemented using the plan as context
- [ ] Updated `docs/character-detail-challenge.md` with final metrics
- [ ] Can articulate the difference between planning and implementing with AI

#### ‚ú® The "After" ‚Äî The Complete Transformation

**Compare your journey:**

| Attempt | Context | Approach | Typical Result |
|---------|---------|----------|----------------|
| Module 00 | None | Direct ask | Chaos, 3+ attempts |
| Module 01a | ARCHITECTURE.md | Direct ask | Better structure, still iterations |
| Module 01b | + Instructions | Direct ask | Consistent patterns, fewer iterations |
| **Module 02** | + Plan Mode | **Plan then implement** | **First-try success, edge cases covered** |

With structured planning, you now have:
- **Clear requirements** that you've thought through with AI assistance
- **Implementation roadmap** that considers architecture and edge cases
- **Team documentation** that others can review and build upon
- **Confidence** in your approach before writing any code
- **Measurable improvement** across all your attempts

**Time invested**: 20 minutes of planning  
**Value unlocked**: First-try implementation without backtracking  
**Edge cases identified upfront**: 7 (vs 1-2 discovered during iterations)  
**Confidence level**: 9/10 (vs 3-7 in previous attempts)

#### üìö Official Docs

- [GitHub Copilot: Agent Mode](https://docs.github.com/en/copilot/using-github-copilot/copilot-chat/copilot-chat-in-github)
- [VS Code: Copilot Chat Modes](https://code.visualstudio.com/docs/copilot/copilot-chat#_chat-modes)
- [GitHub Docs: Best practices for prompting GitHub Copilot Chat](https://docs.github.com/en/copilot/using-github-copilot/best-practices-for-using-github-copilot)

#### üí≠ Sarah's Realization

*"I've been skeptical of AI tools because they felt chaotic. But plan mode isn't chaotic‚Äîit's structured thinking. This is how AI actually helps experienced developers: by amplifying our planning, not replacing our judgment."*

#### üé≠ The Team Celebrates

**Sarah**: *"Look at the progression. Same prompt, dramatically different results. THIS is what I needed to see."*

**David**: *"Planning isn't overhead‚Äîit's where my 20 years of experience matters most. The AI handles syntax; I handle strategy."*

**Priya**: *"I never would have thought about those edge cases on my own. Plan mode taught me to think like a senior developer!"*

**Marcus**: *"This is like code review BEFORE you write the code. Catch the issues when they're cheap to fix."*

---

### Exercise 2.2: Iterate on Configuration ‚Äî "David Designs an Architecture Assistant"

**‚è±Ô∏è Time**: ~25 minutes  
**Lead Persona**: David (Seasoned Architect)  
**Supporting Cast**: Sarah (sprint lead), Marcus (systematization perspective)

#### üìñ The Story

**David** (Seasoned Architect, 20 years) has a confession: He's been worried that AI tools might make his architectural expertise less valuable. But after seeing plan mode in action, he's realized something important.

*"AI doesn't replace architectural thinking‚Äîit amplifies it,"* David reflects. *"But only if I use it as a thinking partner, not just a code generator."*

Today, David will use plan mode to design a custom Copilot agent that embodies his architectural expertise‚Äîshowing that the deepest knowledge makes AI tools more powerful, not less relevant.

**[See David's full journey ‚Üí](personas/david.md)**

#### ‚ùå The "Before" ‚Äî Generic Architectural Advice

When David asks Copilot about architecture decisions, he often gets generic advice:

*"Use microservices for scalability."*  
*"Consider caching for performance."*  
*"Add error handling."*

These aren't wrong, but they're not contextual to his specific project, constraints, or architectural philosophy. The AI gives standard answers instead of reasoning through his particular situation:
- His tech stack (React, Node.js, PostgreSQL)
- His constraints (3-person team, ship fast, prioritize maintainability)
- His philosophy (simple over clever, consistency over optimization)
- His project context (TV fan site, read-heavy workload)

**Result**: David spends 10-15 minutes per question filtering generic advice for relevance.

#### üéØ Objective

Use plan mode to iteratively design a custom Copilot agent that reflects David's architectural expertise and project context, creating a reusable artifact for Modules 04 and 07.

#### üìã Steps

1. **Experience the "before" state**
   
   Ask Copilot a generic architecture question:
   
   ```
   How should I handle error cases in the FanHub API?
   ```
   
   Notice the generic response‚Äîit might be technically correct but lacks the context of your specific requirements, constraints, and architectural philosophy.

2. **Use plan mode to design a custom agent**
   
   Switch to agent plan mode to design a better approach:
   
   ```
   @agent I want to create a custom Copilot agent that provides architecture 
   guidance specifically for our FanHub project. Help me plan this agent's 
   design.
   
   The agent should:
   - Understand our tech stack (React, Node.js, PostgreSQL)
   - Know our constraints (small team, need to ship fast, prioritize maintainability)
   - Follow our architectural principles (simple over clever, consistency over optimization)
   - Consider our specific context (TV show fan site, read-heavy workload, user-generated content)
   
   What questions should I answer to create an effective custom agent prompt?
   ```

3. **Engage with the planning conversation**
   
   Plan mode will ask you questions to help design the agent:
   - What architectural decisions does your team face most often?
   - What are your preferred patterns for this tech stack?
   - What anti-patterns do you want the agent to warn against?
   - How technical should the guidance be?
   
   **This is where David's 20 years of experience becomes the input for better AI output.**

4. **Iteratively refine the agent design**
   
   Based on the planning conversation, ask for a draft agent prompt:
   
   ```
   Create a draft custom agent prompt that incorporates our discussion. 
   The agent should be named "Architecture-Assistant" and provide 
   context-aware architectural guidance for the FanHub project.
   ```
   
   Review the draft and iterate:
   
   ```
   The agent prompt is too generic. Make it more specific to our constraints:
   - We have a 3-person team, so solutions need to be maintainable by small teams
   - We're prioritizing shipping over perfect architecture
   - We want pragmatic advice, not theoretical best practices
   ```

5. **Test and refine the agent concept**
   
   Plan mode can help you think through how this agent would work:
   
   ```
   How would this custom agent handle the original question: 
   "How should I handle error cases in the FanHub API?"
   
   Show me how the response would be different with our custom context.
   ```

6. **Save your agent design**
   
   Document your planning work for use in Modules 04 and 07:
   
   ```
   Create fanhub/docs/AGENT-architecture-assistant.md documenting:
   - The custom agent prompt we developed
   - Our architectural principles and constraints
   - Examples of how responses improve with context
   - Notes for Module 04 (custom instructions) and Module 07 (full custom agent)
   ```

#### ‚úÖ Success Criteria

- [ ] Used plan mode to think through agent design systematically
- [ ] Identified specific architectural context and constraints  
- [ ] Iteratively refined the agent concept based on planning feedback
- [ ] Created `fanhub/docs/AGENT-architecture-assistant.md` for Modules 04/07
- [ ] Can explain how expertise makes AI tools more powerful

#### ‚ú® The "After" ‚Äî Expertise Amplification

Through structured planning, David discovered:

**Architecture advice quality:**
- **Before**: Generic patterns, 10-15 min filtering per question
- **After**: Context-specific guidance, < 2 min to actionable advice

**Key metrics:**
- **Time to design agent**: 25 minutes
- **Architectural knowledge captured**: 20 years distilled into agent design
- **Team benefit**: Every AI interaction now considers architectural constraints
- **Modules using this artifact**: 2 (Modules 04 and 07)

**The profound realization**: Junior developers with AI can write code. But **only experienced architects can design AI agents that think architecturally.** Your 20 years of knowledge isn't obsolete‚Äîit's what makes you able to create tools that embody architectural wisdom.

#### üìö Official Docs

- [GitHub Docs: Using GitHub Copilot Chat](https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide)
- [VS Code: Copilot Customization](https://code.visualstudio.com/docs/copilot/copilot-customization)
- [GitHub Docs: About customizing Copilot](https://docs.github.com/en/copilot/customizing-copilot/about-customizing-github-copilot)

#### üí≠ David's Transformation

*"I was worried AI would make my architectural knowledge obsolete. But I realize now‚Äîmy 20 years of experience is what makes me able to design AI tools that actually understand architecture. The expertise isn't obsolete, it's the competitive advantage."*

---

### Exercise 2.3: Diagnose Test Failures Systematically ‚Äî "Marcus Investigates the Build"

**‚è±Ô∏è Time**: ~20 minutes  
**Lead Persona**: Marcus (DevOps Developer)  
**Supporting Cast**: Sarah (debugging mentor), Elena (testing perspective)

#### üìñ The Story

**Marcus** (DevOps Developer, 5 years) is comfortable with infrastructure code‚ÄîDocker, Kubernetes, CI/CD pipelines are his domain. But when application tests fail, he feels that familiar discomfort: **he doesn't always understand what the tests validate or why they're failing.**

The CI/CD pipeline shows 3 failing tests after the dev team's latest merge. Marcus's usual approach:
1. Rerun the tests (maybe it's flaky?)
2. Check if dependencies broke
3. Look at test names and... guess?
4. Eventually ask a dev team member for help

**Today's transformation**: Instead of guessing, Marcus will use plan mode to **systematically understand what the tests validate and diagnose why they're failing**.

**[See Marcus's full journey ‚Üí](personas/marcus.md)**

#### ‚ùå The "Before" ‚Äî The Guessing Game

**CI/CD pipeline shows 3 failing tests:**
- `CharacterList.test.js` ‚Üí "renders character cards correctly"
- `Episodes.test.js` ‚Üí "filters episodes by season"  
- `api/characters.test.js` ‚Üí "returns 404 for unknown character"

**Marcus's old approach:**
1. Stare at test names, try to guess what they validate
2. Rerun the pipeline (hoping it's flaky)
3. Check recent commits (who broke it?)
4. Eventually ping dev team: "Hey, tests are failing, can someone look?"

**Time wasted**: 45 minutes  
**Confidence understanding the problem**: 3/10  
**Learning gained**: Minimal (someone else explains and fixes it)

#### üéØ Objective

Use plan mode to systematically understand what the failing tests validate, diagnose the root cause, and document your investigation methodology for future use.

#### üìã Steps

1. **Ask plan mode to explain what the tests validate**
   
   Start with understanding what you're testing:
   
   ```
   @agent The CI pipeline shows 3 failing tests. Before I try to fix them, 
   help me understand what each test validates:
   
   1. CharacterList.test.js - "renders character cards correctly"
   2. Episodes.test.js - "filters episodes by season"
   3. api/characters.test.js - "returns 404 for unknown character"
   
   For each test, explain:
   - What functionality it validates
   - What would cause it to fail
   - What I should check first
   ```

2. **Generate hypotheses about failure causes**
   
   Once you understand what the tests validate, use plan mode to generate hypotheses:
   
   ```
   @agent Based on what these tests validate, what are the most likely 
   causes of failure? The tests started failing after the latest merge 
   to the main branch. Create a hypothesis-driven investigation plan.
   ```
   
   Plan mode might suggest:
   - Recent merge changed data structure (check API response format)
   - Component props changed (check component interfaces)
   - API endpoint URL changed (check route definitions)

3. **Follow the investigation plan**
   
   Execute the investigation strategy:
   
   ```
   @agent Let's investigate the first hypothesis: data structure changes. 
   Compare the current API response format for character data with what 
   the frontend components expect. What mismatches exist?
   ```
   
   **Example findings**:
   - Backend API changed from `character.name` to `character.fullName`
   - Frontend components still expect `character.name`
   - Season filter expects `episode.seasonNumber` but API returns `episode.season`

4. **Document your investigation**
   
   Create a systematic investigation document:
   
   ```
   @agent Create a test failure investigation document that includes:
   
   1. What each test validates (what we're testing)
   2. Hypotheses about failure causes (systematic investigation)
   3. Root cause analysis (what we found)
   4. Fix approach (how to resolve)
   5. Investigation methodology (for future use)
   
   Save this as fanhub/docs/TEST-FAILURE-INVESTIGATION.md
   ```

5. **Identify the fix approach**
   
   With root cause identified, plan the fix:
   
   ```
   @agent Based on the schema mismatch we found, what's the best fix approach?
   Should we:
   - Update frontend to use new field names?
   - Update backend to maintain backward compatibility?
   - Create a migration strategy?
   
   Consider our constraints: 3-person team, need to ship fast.
   ```

#### ‚úÖ Success Criteria

- [ ] Used plan mode to understand what each test validates
- [ ] Generated systematic hypotheses about failure causes
- [ ] Followed investigation plan to identify root cause (schema mismatch)
- [ ] Created `fanhub/docs/TEST-FAILURE-INVESTIGATION.md` with methodology
- [ ] Can explain investigation approach without guessing

#### ‚ú® The "After" ‚Äî Systematic Investigation

**Marcus's transformation:**

| Metric | Before (Guessing) | After (Systematic) |
|--------|-------------------|---------------------|
| **Time to understand failure** | 45 minutes | 20 minutes |
| **Confidence** | 3/10 | 8/10 |
| **Approach** | Trial-and-error | Hypothesis-driven |
| **Learning** | Low (ask for help) | High (understand root cause) |

**Investigation breakdown:**
- **5 minutes**: Understand what tests validate
- **5 minutes**: Generate hypotheses about causes
- **5 minutes**: Investigate and find root cause
- **5 minutes**: Document investigation and fix approach

**Total**: 20 minutes to fully understand the problem (vs 45 minutes guessing + asking for help)

**Artifact created**: `TEST-FAILURE-INVESTIGATION.md` becomes investigation template for future debugging and is referenced in Elena's testing work in Module 03.

#### üìö Official Docs

- [VS Code: Copilot Chat](https://code.visualstudio.com/docs/copilot/copilot-chat)
- [GitHub Docs: Using GitHub Copilot Chat in your IDE](https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide)
- [GitHub Docs: Getting started with Copilot Chat](https://docs.github.com/en/copilot/github-copilot-chat/copilot-chat-in-ides/using-github-copilot-chat-in-your-ide)

#### üí≠ Marcus's Transformation

*"I used to feel like tests were a black box‚Äîthey pass or they fail, and I don't know why. But plan mode helped me understand what each test validates and systematically investigate failures. I'm not just running pipelines anymore‚ÄîI understand what I'm testing."*

---

### Exercise 2.4: Debug Investigation ‚Äî "Sarah Investigates Before Fixing"

**‚è±Ô∏è Time**: ~10 minutes  
**Lead Persona**: Sarah (Skeptical Senior)  
**Supporting Cast**: Marcus (CI/CD health), David (architectural review)

#### üìñ The Story

**Sarah** (Skeptical Senior, 15 years) gets a bug report: *"The Episodes page shows wrong episodes sometimes. I click Season 2, but I see Season 1 episodes. Refreshing fixes it."*

Sarah's instinct is to dive into the code and start changing things. But she remembers: **plan mode helps you think before you act.**

*"If I understand the bug first, I'll fix it once instead of creating three new bugs."*

**[See Sarah's full journey ‚Üí](personas/sarah.md)**

#### ‚ùå The "Before" ‚Äî The Debugging Anti-Pattern

Without planning, debugging often looks like:
1. Read the bug report, make assumptions
2. Jump to the code, start changing things
3. "Fix" one thing, break another
4. Spend 2 hours on a 15-minute bug

**Symptoms of this bug:**
- Intermittent (doesn't happen every time)
- Stale data (wrong season episodes displayed)
- Refresh fixes it (suggests caching issue)

#### üéØ Objective

Use plan mode to systematically investigate the bug before writing any fix.

#### üìã Steps

1. **Ask plan mode to help investigate**
   
   ```
   @agent Users report the Episodes page sometimes shows wrong data‚Äîthey 
   click Season 2 but see Season 1 episodes. It's intermittent and 
   refreshing fixes it.
   
   Before I change any code, help me investigate:
   1. What could cause intermittent, stale data issues?
   2. What should I look for in the Episodes.js component?
   3. What's my hypothesis-driven investigation plan?
   ```

2. **Follow the investigation plan**
   
   Plan mode will likely point you toward:
   - **Caching issues** ‚Äî Is there a cache that's not invalidated?
   - **Race conditions** ‚Äî Is data loading before filters apply?
   - **State management** ‚Äî Is state persisting when it shouldn't?
   
   Open [fanhub/frontend/src/pages/Episodes.js](../../fanhub/frontend/src/pages/Episodes.js) and look for what plan mode identified.

3. **Identify the root cause**
   
   ```
   @agent I found a module-level cache (episodeCache) and cacheTimestamp 
   in Episodes.js. Analyze whether this could cause the reported bug.
   ```
   
   **Root cause discovery**: The cache doesn't track which season was requested‚Äîit caches ALL episode data regardless of season filter.

4. **Document your findings**
   
   ```
   @agent Create a brief bug analysis document that explains:
   - Root cause (module-level cache with no season tracking)
   - Why it's intermittent (cache hits vs misses)
   - Recommended fix approach (remove cache OR make it season-aware)
   
   Save as fanhub/docs/BUG-ANALYSIS-episode-cache.md
   ```

#### ‚úÖ Success Criteria

- [ ] Used plan mode to create an investigation strategy
- [ ] Identified the caching bug in `Episodes.js`
- [ ] Created `fanhub/docs/BUG-ANALYSIS-episode-cache.md`
- [ ] Understood the bug **before** attempting to fix it

#### ‚ú® The "After" ‚Äî Systematic Debugging

Instead of 2 hours of trial-and-error, Sarah spent:
- **5 minutes** creating an investigation plan
- **5 minutes** following the plan to find the bug  
- **5 minutes** documenting the root cause

**Total**: 15 minutes to fully understand an intermittent bug

**The fix is now trivial** because the problem is understood. Options:
1. Remove the flawed cache (simplest)
2. Make cache season-aware (more complex)

**Key insight**: Investigation time is an investment. Understand the problem, and the fix is obvious.

#### üí≠ Sarah's Validation

*"This is exactly what I needed to see. Plan mode isn't just for new features‚Äîit's for thinking through problems. I've been debugging for 15 years, but having AI help structure my investigation? That's actually useful."*

#### üöÄ Challenge Extension

If you want to complete the fix:
1. Remove the flawed `episodeCache` and `cacheTimestamp` variables
2. Or implement proper caching with season-aware keys
3. Test by rapidly switching between seasons

#### üìö Official Docs

- [GitHub Docs: Using GitHub Copilot Chat in your IDE](https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide)
- [VS Code: Copilot Chat Context](https://code.visualstudio.com/docs/copilot/copilot-chat#_chat-context)
- [GitHub Docs: Using chat participants](https://docs.github.com/en/copilot/github-copilot-chat/copilot-chat-in-ides/using-github-copilot-chat-in-your-ide#using-chat-participants)

---

## üßµ Golden Thread Checkpoint: The Complete Character Detail Journey

You've now completed the **Character Detail Challenge** across four iterations:

### Evolution Table

| Module | Context Available | Approach | Typical Result |
|--------|-------------------|----------|----------------|
| **00: Orientation** | Nothing | Direct ask | Chaos, 4-5 attempts, generic code, inconsistent patterns |
| **01a: Architecture** | ARCHITECTURE.md | Direct ask | Better structure, 2-3 attempts, correct placement |
| **01b: Instructions** | + copilot-instructions.md | Direct ask | Consistent patterns, 1-2 attempts, matches standards |
| **02: Plan Mode** | + Plan thinking | **Plan then implement** | **First-try success, 7 edge cases identified upfront** |

### Key Metrics Progression

| Metric | Mod 00 | Mod 01a | Mod 01b | **Mod 02** |
|--------|--------|---------|---------|------------|
| **Time to working code** | 60+ min | 40 min | 25 min | **20 min plan + first-try** |
| **Prompt attempts** | 4-5 | 2-3 | 1-2 | **1 (with comprehensive plan)** |
| **Edge cases identified** | 1-2 | 3-4 | 4-5 | **7 (upfront in planning)** |
| **Confidence (1-10)** | 3 | 6 | 7 | **9** |
| **Iterations needed** | 3-4 | 2 | 1-2 | **0 (first-try success)** |
| **Team collaboration** | Solo struggle | Shared docs | Shared standards | **Shared plans + methodology** |

### Persona Perspectives on the Golden Thread

**Sarah (Skeptical Senior)** ‚Äî *Validation lens*:
- Module 00: "This is chaos. Prove this works."
- Module 01: "Better, but still reactive."
- Module 02: "20 minutes of planning ‚Üí first-try success. That's measurable ROI."

**David (Seasoned Architect)** ‚Äî *Architecture lens*:
- Module 00: "No architectural thinking at all."
- Module 01: "Structure exists, but planning is ad-hoc."
- Module 02: "Edge cases identified upfront. Planning is where my expertise shines."

**Marcus (DevOps Developer)** ‚Äî *Understanding lens*:
- Module 00: "I don't understand what this code does."
- Module 01: "I can follow the patterns now."
- Module 02: "I can explain every design decision because we planned it."

### The Compounding Lesson

> *"Same prompt. Dramatically different results. That's not magic‚Äîthat's engineering."* ‚Äî Sarah

Each layer of investment compounds:
1. **Documentation** provides context
2. **Instructions** provide patterns
3. **Plan mode** provides strategy

Together, they transform AI collaboration from frustrating guesswork into systematic success.

---

## üîó Compounding Value

### Artifacts Created in Module 02

| Artifact | Purpose | Used In |
|----------|---------|---------|
| `FEATURE-PLAN-character-detail.md` | Implementation plan for Character Detail page | Module 02 implementation |
| `AGENT-architecture-assistant.md` | Custom agent design embodying architectural expertise | **Modules 04 (custom instructions) & 07 (custom agents)** |
| `TEST-FAILURE-INVESTIGATION.md` | Systematic test debugging methodology | **Module 03 (Elena's testing work)** |
| `BUG-ANALYSIS-episode-cache.md` | Root cause analysis template | Future debugging reference |
| `character-detail-challenge.md` (updated) | Complete Golden Thread metrics | Module 02 validation |

### How These Connect to Future Modules

**Module 03 (Custom Prompts)**:
- Marcus's investigation template ‚Üí Elena creates test generation prompts
- Planning methodology ‚Üí Systematic prompt design

**Module 04 (Custom Instructions)**:
- David's architecture assistant ‚Üí Repository-level custom instructions
- Planning approach ‚Üí Instruction design workflow

**Module 07 (Custom Agents)**:
- David's architecture assistant ‚Üí Full custom agent with workflows
- Planning methodology ‚Üí Agent capability design

---

## üß† Mindful Moment: From Syntax Wizards to Strategic Thinkers

**Old paradigm**: Developer value = memorizing syntax and APIs

**New paradigm**: Developer value = clarity of thought and strategic planning

Plan mode doesn't replace thinking‚Äîit **amplifies** it. The clearer your thinking, the better your AI results. This is why experienced developers get MORE value from AI tools, not less:
- **20 years of experience** ‚Üí Better questions to ask
- **Deep domain knowledge** ‚Üí More contextual requirements
- **Systematic thinking** ‚Üí More structured plans

**AI handles syntax. You handle strategy.**

---

## ‚úÖ Module Checklist

- [ ] Completed Exercise 2.1: Character Detail Challenge with plan mode
- [ ] Updated `docs/character-detail-challenge.md` with Module 02 metrics
- [ ] Completed Exercise 2.2: Architecture assistant design
- [ ] Created `fanhub/docs/AGENT-architecture-assistant.md` for Modules 04/07
- [ ] Completed Exercise 2.3: Systematic test failure investigation
- [ ] Created `fanhub/docs/TEST-FAILURE-INVESTIGATION.md`
- [ ] Completed Exercise 2.4: Systematic bug investigation
- [ ] Created `fanhub/docs/BUG-ANALYSIS-episode-cache.md`
- [ ] Can explain plan mode vs implementation mode
- [ ] Can articulate how structured thinking improves AI output

---

## üìö Official Documentation

- [GitHub Copilot: Agent Mode](https://docs.github.com/en/copilot/using-github-copilot/copilot-chat/copilot-chat-in-github)
- [VS Code: Copilot Chat Modes](https://code.visualstudio.com/docs/copilot/copilot-chat#_chat-modes)
- [GitHub Docs: Best practices for prompting GitHub Copilot Chat](https://docs.github.com/en/copilot/using-github-copilot/best-practices-for-using-github-copilot)

---

## ‚û°Ô∏è Next Up

**[Module 3: Custom Prompts](../03-custom-prompts/README.md)** ‚Äî Reusable AI Instructions

> *"What if we could save our best AI conversations and reuse them for every similar task?"*  
> ‚Äî Elena, discovering prompt engineering

In Module 03, you'll build on your plan mode foundation by creating custom prompts that embody your team's patterns and preferences. You'll discover how the planning skills from Module 02 make prompt engineering more effective and systematic.

**Preview**: Elena will use Marcus's test investigation methodology to create custom prompts for test generation. Your planning artifacts become the inputs for reusable AI instructions.

---

## üé≠ Behind the Scenes: The Planning Transformation

**Module 02's role in the 9-module story arc**:

- **Module 00**: Chaos (no context, no planning)
- **Module 01**: Structure (documentation provides context)
- **Module 02**: ‚≠ê **Strategy (plan mode provides thinking framework)** ‚≠ê
- **Modules 03-09**: Build on strategic planning foundation

Every subsequent module benefits from plan mode thinking:
- Module 03: Plan custom prompts before creating them
- Module 04: Plan custom instructions before implementing
- Module 05: Plan agent skills systematically
- Module 07: Plan custom agent design (using David's Module 02 artifact)

**The meta-lesson**: Plan mode isn't just a Module 02 technique‚Äîit's a thinking approach that makes everything else more effective.
